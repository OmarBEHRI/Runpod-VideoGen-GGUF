MODELS:

Diffusion models: in /ComfyUI/models/unet
Wan2.2-I2V-A14B-HighNoise-Q5_0.gguf : https://huggingface.co/QuantStack/Wan2.2-I2V-A14B-GGUF/resolve/main/HighNoise/Wan2.2-I2V-A14B-HighNoise-Q5_0.gguf?download=true
Wan2.2-I2V-A14B-LowNoise-Q5_0.gguf : https://huggingface.co/QuantStack/Wan2.2-I2V-A14B-GGUF/resolve/main/LowNoise/Wan2.2-I2V-A14B-LowNoise-Q5_0.gguf?download=true

Use these instead of Q3_K_L models

VAE: /ComfyUI/models/vae
Wan2.1_VAE.safetensors : https://huggingface.co/QuantStack/Wan2.2-I2V-A14B-GGUF/resolve/main/VAE/Wan2.1_VAE.safetensors?download=true

Text Encoders: /ComfyUI/models/text_encoders
umt5_xxl_fp8_e4m3fn_scaled.safetensors : https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors

Loras: /ComfyUI/models/Loras
lightx2v_I2V_14B_480p_cfg_step_distill_rank64_bf16.safetensors : https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/Lightx2v/lightx2v_I2V_14B_480p_cfg_step_distill_rank64_bf16.safetensors?download=true

RIFE Model:
The rife model is downloaded and used automatically


CCUSTOM NODES:
Clone the following github repos in the following location: ComfyUI/custom_nodes
https://github.com/Comfy-Org/ComfyUI-Manager.git
https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite
https://github.com/Fannovel16/ComfyUI-Frame-Interpolation
https://github.com/city96/ComfyUI-GGUF

INPUT:
the endpoint should take as input:
- An input image
- A prompt for the image animation
- Negative prompt (default settings will be used)
- Video length in seconds or desired frame count
- Frame rate (FPS) settings
- Video output format (e.g., MP4, WebM, GIF)





